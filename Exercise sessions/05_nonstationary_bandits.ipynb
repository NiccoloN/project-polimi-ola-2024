{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-stationary Bandits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a stochastic non-stationary environment, the mean of the distributions from which rewards are sampled __changes__ over time. Thus, to define a non-stationary environment for K-armed bandit problem, we need a set a functions that describe the evolution of the average rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE**: we will only deal with piecewise-constant functions to describe the evolution of average rewards, since they are easier to deal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "K = 3\n",
    "\n",
    "# we need to define multiple sequences of rewards and be able to access them comfortably\n",
    "mu = ([0.35]*2500 + [0.65]*7500,\n",
    "        [0.25]*3500 + [0.5]*2500 + [0.8]*4000,\n",
    "        [0.2]*3000 + [0.7]*4000 + [0.75]*3000\n",
    "    )\n",
    "# we define a function that goes from [1,...,T] -> [0,1]^3\n",
    "# it is a good practice to keep rewards (or at least their average) in [0,1] also in non-stationary environments\n",
    "t = np.arange(T)\n",
    "plt.plot(t, np.c_[mu], label=[f'$\\mu_{i}$' for i in range(K)])\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: the functions doesn't need to be non-decreasing, they can vary arbitrarily (even decrease). FYI, non-stationary bandits where average rewards never decrease are called _rising_ bandits. On the contrary, if average rewards never increase we refer to them as _rotting_ bandits. Those are easier class of problems where ad-hoc algorithms can be used to obtain better performances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the time passes, the average rewards change their means. Imagine to face this bandit problem as an adversarial one: what would be the best policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# Best-arm in Hindsight:\n",
    "cum_rew = np.array(mu).sum(axis=1)\n",
    "best_arm = np.argmax(cum_rew)\n",
    "print(f'Cumulative rewards for every arm: {cum_rew}')\n",
    "print(f'Best arm in hindsight: {best_arm}')\n",
    "\n",
    "for i in range(K):\n",
    "    if i == best_arm:\n",
    "        plt.plot(t, mu[i], label=f'$\\mu_{i} = \\mu^*$', c='red')\n",
    "    else:\n",
    "        plt.plot(t, mu[i], label=f'$\\mu_{i}$', c=f'C{i}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A no-regret adversarial learner would have its performance compared to the clairvoyant that __always__ pulls arm 2.\n",
    "\n",
    "### In non-stationary stochastic environments, we look for a stronger notion of regret, which is the _policy regret_: the learner is compared to the best possible __policy__ in hindsight, i.e., the best possible sequence of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Best policy in Hindsight:\n",
    "best_rewards = np.array(mu).max(axis=0) # we take the max over every single round\n",
    "best_cum_rew = sum(best_rewards)\n",
    "best_policy = np.array(mu).argmax(axis=0)\n",
    "print(f'Best possible cumulative reward: {best_cum_rew}') # is higher than the cumulative reward of the best arm in hindsight\n",
    "\n",
    "for i in range(K):\n",
    "        plt.plot(t, mu[i], label=f'$\\mu_{i}$')\n",
    "plt.plot(t, best_rewards, label=f'$\\mu^*$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that \n",
    "### $$ \\underbrace{\\max_{a} \\sum_{t=1}^T \\mu_a(t)}_{\\text{best arm in hindsight cumulative reward (adversarial clairvoyant)}} \\le \\underbrace{\\sum_{t=1}^T \\max_a \\mu_a(t)}_{\\text{best policy cumulative reward (non-stationary stochastic clairvoyant)}},$$\n",
    "### where $=$ holds if there's an arm dominating all the other in every single round."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Stationary Stochastic Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: it makes no sense to talk of Non-Stationary _Adversarial_ environment, every Non-Stationary Stochastic environment is also an adversarial environment, what changes is the way to evaluate a learner's performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the average reward functions are in [0,1], we can use them as the probability of success in sampling from a Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# similar implementation of the stationary scenario\n",
    "class NonstationaryBernoulliEnvironment:\n",
    "    def __init__(self, mu, T, seed):\n",
    "        np.random.seed(seed)\n",
    "        self.mu = np.array(mu)\n",
    "        self.rewards = np.random.binomial(n=1, p=self.mu.T)\n",
    "        self.K = self.rewards.shape[1]\n",
    "        self.t = 0\n",
    "\n",
    "    def round(self, a_t):\n",
    "        r_t = self.rewards[self.t, a_t]\n",
    "        self.t +=1\n",
    "        return r_t"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "env = NonstationaryBernoulliEnvironment(mu, T, seed=17)\n",
    "y = env.rewards\n",
    "plt.plot(t, env.rewards.cumsum(axis=0), label=[f'$\\sum_s^t X_s,_{i}$' for i in range(K)])\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.title('Cumulative Sampled Rewards')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we expect UCB1 to perform on this environment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "class UCB1Agent:\n",
    "    def __init__(self, K, T, range=1):\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.range = range\n",
    "        self.a_t = None\n",
    "        self.average_rewards = np.zeros(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        if self.t < self.K:\n",
    "            self.a_t = self.t \n",
    "        else:\n",
    "            ucbs = self.average_rewards + self.range*np.sqrt(2*np.log(self.T)/self.N_pulls)\n",
    "            self.a_t = np.argmax(ucbs)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.average_rewards[self.a_t] += (r_t - self.average_rewards[self.a_t])/self.N_pulls[self.a_t]\n",
    "        self.t += 1"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same code from Lab 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "n_epochs = 20\n",
    "mu = ([0.35]*2500 + [0.65]*7500,\n",
    "        [0.25]*3500 + [0.5]*2500 + [0.8]*4000,\n",
    "        [0.2]*3000 + [0.7]*4000 + [0.75]*3000\n",
    "    )\n",
    "x = np.arange(T)\n",
    "K = len(y)\n",
    "\n",
    "expected_clairvoyant_rewards = np.array(mu).max(axis=0)\n",
    "\n",
    "n_trials = 100\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = NonstationaryBernoulliEnvironment(mu, T, seed)\n",
    "    ucb_agent = UCB1Agent(K, T)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ucb_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ucb_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('cumulative regret of UCB1')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of UCB1')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the regret has a convex shape? -> very important question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window Strategies - SW-UCB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: try to adapt to environment's changes by only considering most recent rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "source": [
    "class SWUCBAgent:\n",
    "    def __init__(self, K, T, W, range=1):\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.W = W\n",
    "        self.range = range\n",
    "        self.a_t = None\n",
    "        self.cache = np.repeat(np.nan, repeats=K*W).reshape(W, K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        if self.t < self.K:\n",
    "            self.a_t = self.t \n",
    "        else:\n",
    "            n_pulls_last_w = self.W - np.isnan(self.cache).sum(axis=0)\n",
    "            avg_last_w = np.nanmean(self.cache, axis=0)\n",
    "            ucbs = avg_last_w + self.range*np.sqrt(2*np.log(self.W)/n_pulls_last_w) # there's a typo in the slides, log(T) -> log(W)\n",
    "            self.a_t = np.argmax(ucbs)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.cache = np.delete(self.cache, (0), axis=0) # remove oldest observation\n",
    "        new_samples = np.repeat(np.nan, self.K)\n",
    "        new_samples[self.a_t] = r_t\n",
    "        self.cache = np.vstack((self.cache, new_samples)) # add new observation\n",
    "        self.t += 1"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose $W$ according to theory:\n",
    "### $$ W = \\left \\lfloor 2B\\sqrt{T \\log T / \\Upsilon_T} \\right \\rfloor,$$\n",
    "### where $B$ is the maximum reward ($B=1$ for Bernoulli), and $\\Upsilon_T$ is the maximum number of times an arm changes its average reward. \n",
    "### Reference: https://arxiv.org/pdf/0805.3415 (Remark 9)\n",
    "### **NOTE**: this choice only works for environments with abrupt changes, to work on smoothly changing environments some modifications to the algorithm are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "n_epochs = 20\n",
    "mu = ([0.35]*2500 + [0.65]*7500,\n",
    "        [0.25]*3500 + [0.5]*2500 + [0.8]*4000,\n",
    "        [0.2]*3000 + [0.7]*4000 + [0.75]*3000\n",
    "    )\n",
    "K = len(mu)\n",
    "U_T = 3 # maximum number of abrupt changes\n",
    "W = int(2*np.sqrt(T*np.log(T)/U_T)) # assuming U_T is known\n",
    "# W = int(2*np.sqrt(np.log(T))) # if U_T is unknown (i.e., set U_T=T)\n",
    "\n",
    "expected_clairvoyant_rewards = np.array(mu).max(axis=0)\n",
    "\n",
    "n_trials = 20\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = NonstationaryBernoulliEnvironment(mu, T, seed)\n",
    "    ucb_agent = SWUCBAgent(K, T, W)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ucb_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ucb_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('cumulative regret of SW-UCB')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of SW-UCB')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sublinear regret!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: code Sliding Window Thompson Sampling and compare its performances with SW-UCB and UCB1 on this environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference fon SW-TS: https://www.jair.org/index.php/jair/article/view/11407/26587 (Algorithm 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: set $W$ as $\\sqrt{T}$ it the number of breakpoints is negligible w.r.t. to $T$ (i.e., $\\Upsilon_T << T$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Detection Approaches - CUSUM-UCB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea: play with an algorithm for stationary environments (e.g., UCB1) until you detect a change in the average reward of a specific arm, then reset the algorithm's knowledge on that arm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://arxiv.org/pdf/1711.03539 (Algorithm 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "source": [
    "class CUSUMUCBAgent:\n",
    "    def __init__(self, K, T, M, h, alpha=0.99, range=1):\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.h = h\n",
    "        self.alpha=alpha\n",
    "        self.range = range\n",
    "        self.a_t = None\n",
    "        self.reset_times = np.zeros(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.all_rewards = [[] for _ in np.arange(K)]\n",
    "        self.counters = np.repeat(M, K)\n",
    "        self.average_rewards = np.zeros(K)\n",
    "        self.n_resets = np.zeros(K)\n",
    "        self.n_t = 0\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        if (self.counters > 0).any():\n",
    "            for a in np.arange(self.K):\n",
    "                if self.counters[a] > 0:\n",
    "                    self.counters[a] -= 1\n",
    "                    self.a_t = a\n",
    "                    break\n",
    "        else:\n",
    "            if np.random.random() <= 1-self.alpha:\n",
    "                ucbs = self.average_rewards + self.range*np.sqrt(np.log(self.n_t)/self.N_pulls)\n",
    "                self.a_t = np.argmax(ucbs)\n",
    "            else:\n",
    "                self.a_t = np.random.choice(np.arange(self.K)) # extra exploration\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.all_rewards[self.a_t].append(r_t)\n",
    "        if self.counters[self.a_t] == 0:\n",
    "            if self.change_detection():\n",
    "                self.n_resets[self.a_t] +=1 \n",
    "                self.N_pulls[self.a_t] = 0\n",
    "                self.average_rewards[self.a_t] = 0\n",
    "                self.counters[self.a_t] = self.M\n",
    "                self.all_rewards[self.a_t] = []\n",
    "                self.reset_times[self.a_t] = self.t \n",
    "            else:\n",
    "                self.average_rewards[self.a_t] += (r_t - self.average_rewards[self.a_t])/self.N_pulls[self.a_t]\n",
    "        self.n_t = sum(self.N_pulls)\n",
    "        self.t += 1\n",
    "\n",
    "    def change_detection(self):\n",
    "        ''' CUSUM CD sub-routine. This function returns 1 if there's evidence that the last pulled arm has its average reward changed '''\n",
    "        u_0 = np.mean(self.all_rewards[self.a_t][:self.M])\n",
    "        sp, sm = (np.array(self.all_rewards[self.a_t][self.M:])- u_0, u_0 - np.array(self.all_rewards[self.a_t][self.M:]))\n",
    "        gp, gm = 0, 0\n",
    "        for sp_, sm_ in zip(sp, sm):\n",
    "            gp, gm = max([0, gp + sp_]), max([0, gm + sm_])\n",
    "            if max([gp, gm]) >= self.h:\n",
    "                return True\n",
    "        return False\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "n_epochs = 20\n",
    "mu = ([0.35]*2500 + [0.65]*7500,\n",
    "        [0.25]*3500 + [0.5]*2500 + [0.8]*4000,\n",
    "        [0.2]*3000 + [0.7]*4000 + [0.75]*3000\n",
    "    )\n",
    "K = len(mu)\n",
    "U_T = 3 # maximum number of abrupt changes\n",
    "h = 2*np.log(T/U_T) # sensitivity of detection, threshold for cumulative deviation\n",
    "alpha = np.sqrt(U_T*np.log(T/U_T)/T) # probability of extra exploration\n",
    "\n",
    "M = int(np.log(T/U_T)) # robustness of change detection\n",
    "# M = int(np.sqrt(T)) # usually keep M in [log(T/U_T), sqrt(T/U_T)]\n",
    "\n",
    "expected_clairvoyant_rewards = np.array(mu).max(axis=0)\n",
    "\n",
    "n_trials = 10\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = NonstationaryBernoulliEnvironment(mu, T, seed)\n",
    "    ucb_agent = CUSUMUCBAgent(K, T, M, h, alpha)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ucb_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ucb_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('cumulative regret of CUSUM-UCB')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of CUSUM-UCB')\n",
    "plt.show();\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.n_resets)\n",
    "plt.title('number of resets per arm of CUSUM-UCB')\n",
    "plt.show();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSUM-UCB achieves comparable performances w.r.t. SW-UCB in this scenario. However there are several drawbacks:\n",
    "### - more hyper-parameters\n",
    "### - additional assumptions (see paper)\n",
    "### - heavier computational burden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks: \n",
    "###   1. In environments with abrupt changes, regret usually scales with the maximum number of changes in an arm $\\Upsilon_T$\n",
    "###   2. The prior knowledge on $\\Upsilon_T$ may improve algorithms' performances, since it can be used to set hyper-parameters sharply\n",
    "###   3. In online learning there is really no \"tuning\", since we cannot evaluate different hyper-parameters on the same trial (which is assumed to be a \"one-shot\" trial). Thus, hyper-parameters choice is only driven by theory and by an eventual prior knowledge on the environment (i.e., $\\Upsilon_T$)\n",
    "###   4. In smoothly changing environments the role of $\\Upsilon_T$ is usually replaced by $V_T$, which is the maximum total variation in the average reward of an arm. Some algorithms can be easily used in both abrupt changes and smooth changes scenarios (such as SW-UCB, with some modifications), while others, such as CUSUM-UCB, are not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
