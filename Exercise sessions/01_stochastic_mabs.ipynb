{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stochastic MABs**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.459048Z",
     "start_time": "2024-05-10T08:08:58.644726Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first thing to do is to code a simulator (or environment) to run a Multi-Armed Bandit Trial. For simplicity, we start with the most simple probability distribution, Bernoulli. Thus, rewards will be $r_t \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.488516Z",
     "start_time": "2024-05-10T08:09:01.468584Z"
    }
   },
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def round(self, a_t):\n",
    "        pass"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.535Z",
     "start_time": "2024-05-10T08:09:01.493510Z"
    }
   },
   "source": [
    "class BernoulliEnvironment(Environment):\n",
    "    def __init__(self, p, T, seed):\n",
    "        np.random.seed(seed)\n",
    "        self.K = len(p)\n",
    "        self.rewards = np.random.binomial(n=1, p=p, size=(T, self.K))\n",
    "        self.t = 0\n",
    "\n",
    "    def round(self, a_t):\n",
    "        r_t = self.rewards[self.t, a_t]\n",
    "        self.t +=1\n",
    "        return r_t"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.596575Z",
     "start_time": "2024-05-10T08:09:01.543695Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "K = len(p)\n",
    "T = 100\n",
    "seed = 17\n",
    "\n",
    "env = BernoulliEnvironment(p, T, seed)\n",
    "env.round(0)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.645349Z",
     "start_time": "2024-05-10T08:09:01.609237Z"
    }
   },
   "source": [
    "env.t, env.round(2), env.t"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I interact with this environment? We use Agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.676142Z",
     "start_time": "2024-05-10T08:09:01.657764Z"
    }
   },
   "source": [
    "# this is the blueprint of an Agent-type class, we can customize this with our strategy\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def pull_arm(self):\n",
    "        pass\n",
    "    def update(self, r_t):\n",
    "        pass"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:01.691977Z",
     "start_time": "2024-05-10T08:09:01.678434Z"
    }
   },
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, K, T, seed):\n",
    "        np.random.seed(seed)\n",
    "        self.actions_sequence = np.random.choice(np.arange(K), size=T)\n",
    "        self.a_t = None\n",
    "        self.action_history = np.array([])\n",
    "        self.t = 0\n",
    "    def pull_arm(self):\n",
    "        self.a_t = self.actions_sequence[self.t]\n",
    "        return self.a_t\n",
    "    def update(self, r_t):\n",
    "        self.action_history = np.append(self.action_history, self.a_t)\n",
    "        self.t += 1"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.177319Z",
     "start_time": "2024-05-10T08:09:01.695321Z"
    }
   },
   "source": [
    "env = BernoulliEnvironment(p, T, seed)\n",
    "rand_agent = RandomAgent(K, T, seed)\n",
    "\n",
    "rewards = np.array([])\n",
    "action_sequence = np.array([])\n",
    "for t in range(T):\n",
    "    a_t = rand_agent.pull_arm()\n",
    "    r_t = env.round(a_t)\n",
    "    rand_agent.update(r_t) # doesn't do anything\n",
    "    #logging\n",
    "    rewards = np.append(rewards, r_t)\n",
    "    action_sequence = np.append(action_sequence, a_t)\n",
    "\n",
    "plt.plot(action_sequence)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$a_t$')\n",
    "plt.title('Chosen Actions')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$r_t$')\n",
    "plt.title('Obtained Rewards')\n",
    "plt.show()"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Notion of Regret**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the best action sequence?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.191951Z",
     "start_time": "2024-05-10T08:09:03.179514Z"
    }
   },
   "source": [
    "best_action = np.argmax(p)\n",
    "print(f'Best action is {best_action}')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The total _expected_ reward of always playing the best action $a^*$ is $T \\mu (a^*) = \\sum_{t=1}^T \\mathbb{E} [r_t(a^*)] = \\mathbb{E} [\\sum_{t=1}^T r_t(a^*)]$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.254122Z",
     "start_time": "2024-05-10T08:09:03.195191Z"
    }
   },
   "source": [
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T)\n",
    "\n",
    "sum(expected_clairvoyant_rewards)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, a clairvoyant doesn't actually gain $T\\mu(a^*)$, but $\\sum_{t=1}^T r_t(a^*)$, this is the actual clairvoyant's reward (and it's random due to the nature of the environment)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.270104Z",
     "start_time": "2024-05-10T08:09:03.256207Z"
    }
   },
   "source": [
    "clairvoyant_rewards = env.rewards[:, best_action]\n",
    "\n",
    "sum(clairvoyant_rewards) "
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Regret express how bad my actions are, _in expectation_, w.r.t. a clairvoyant, i.e. $R_T = \\mathbb{E}[\\sum_{t=1}^T r_t(a^*)-r_t(a_t)] = T\\mu(a^*) - \\sum_{t=1}^T \\mathbb{E}[r_t(a_t)] = \\sum_{t=1}^T \\Delta_{a_t}$, instead, the actual regret is computed on the rewards observed (including the environment's noise)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.286182Z",
     "start_time": "2024-05-10T08:09:03.272258Z"
    }
   },
   "source": [
    "expected_agent_rewards = np.repeat(p.sum()/len(p), T)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:03.856493Z",
     "start_time": "2024-05-10T08:09:03.288691Z"
    }
   },
   "source": [
    "instantaneous_regret = clairvoyant_rewards - rewards\n",
    "instantaneous_pseudo_regret = expected_clairvoyant_rewards - expected_agent_rewards\n",
    "\n",
    "cumulative_regret = np.cumsum(instantaneous_regret)\n",
    "plt.plot(cumulative_regret)\n",
    "plt.xlabel('$t$')\n",
    "plt.title('Cumulative Regret')\n",
    "plt.show()\n",
    "\n",
    "cumulative_pseudo_regret = np.cumsum(instantaneous_pseudo_regret)\n",
    "plt.plot(cumulative_pseudo_regret)\n",
    "plt.xlabel('$t$')\n",
    "plt.title('Cumulative Pseudo Regret')\n",
    "plt.show()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a random agent is easy to compute the expected cumulative reward, and thus to obtain the cumulative pseudo-regret in a closed form. However, for more complex agents is usually impossible to obtain the true expectation on their rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice, after a bandit trial you usually observe only the first plot. But the more interesting is the second one. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we can proceed by trying the agent on multiple different simulations, and empirically estimate its pseudo-regret as the average of all the regrets attained during the trials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need multiple trials to estimate it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:04.224715Z",
     "start_time": "2024-05-10T08:09:03.867566Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "T = 20\n",
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T) # this remains the same after every trial\n",
    "\n",
    "n_trials = 20\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    rand_agent = RandomAgent(K, T, seed)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = rand_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        rand_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('Cumulative regret of Random Player')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "plt.plot((0,T-1), (0, T*(max(p)-p.sum()/len(p))), 'r', linestyle=\"--\", linewidth=0.75)\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We approximated pseudo-regret starting from regrets coming from multiple trials. We also quantified the uncertainty around this estimation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MAIN GOAL IN BANDITS**: Obtain Sub-Linear Regret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to deal with the exploration-exploitation trade-off, but do we really need exploration?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:04.239781Z",
     "start_time": "2024-05-10T08:09:04.227397Z"
    }
   },
   "source": [
    "## Exercise: produce a code showing that a greedy strategy suffers a linear expected regret in certain instances\n",
    "## Hint: code the GreedyAgent class and a new Environment-type class on which to test it"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Explore-Then-Commit (ETC) Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:04.286544Z",
     "start_time": "2024-05-10T08:09:04.241802Z"
    }
   },
   "source": [
    "class ETCAgent(Agent):\n",
    "    def __init__(self, K, T, T0):\n",
    "        self.K = K \n",
    "        self.T = T\n",
    "        self.T0 = T0\n",
    "        self.a_t = None\n",
    "        self.average_rewards = np.zeros(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        if self.t <= self.K*self.T0:\n",
    "            self.a_t = self.t % self.K\n",
    "        else:\n",
    "            self.a_t = np.argmax(self.average_rewards)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.average_rewards[self.a_t] += (r_t - self.average_rewards[self.a_t])/self.N_pulls[self.a_t]\n",
    "        self.t += 1"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will it perform better? Let's estimate the pseudo-regret:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:13.540242Z",
     "start_time": "2024-05-10T08:09:04.288146Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "T = 10000\n",
    "\n",
    "T0 = (T/K)**(2/3)*np.log(T)**(1/3) # we set the exploration length as prescribed by the theory\n",
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T)\n",
    "\n",
    "n_trials = 20\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    etc_agent = ETCAgent(K, T, T0=T0)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = etc_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        etc_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title(f'Cumulative regret of ETC, $T0={round(T0,2)}$')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "#plt.plot((0,T-1), (average_regret[0], average_regret[-1]), 'ro', linestyle=\"--\")\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=etc_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of ETC')"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Will it perform better? Let's estimate the pseudo-regret:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Will it perform better? Let's estimate the pseudo-regret:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Will it perform better? Let's estimate the pseudo-regret:"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do even better?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The UCB1 Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:13.556279Z",
     "start_time": "2024-05-10T08:09:13.542542Z"
    }
   },
   "source": [
    "class UCB1Agent(Agent):\n",
    "    def __init__(self, K, T, range=1):\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.range = range\n",
    "        self.a_t = None\n",
    "        self.average_rewards = np.zeros(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.t = 0\n",
    "    \n",
    "    def pull_arm(self):\n",
    "        if self.t < self.K:\n",
    "            self.a_t = self.t \n",
    "        else:\n",
    "            ucbs = self.average_rewards + self.range*np.sqrt(2*np.log(self.T)/self.N_pulls)\n",
    "            self.a_t = np.argmax(ucbs)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.average_rewards[self.a_t] += (r_t - self.average_rewards[self.a_t])/self.N_pulls[self.a_t]\n",
    "        self.t += 1"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will it perform better? Let's estimate the pseudo-regret:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:18.522039Z",
     "start_time": "2024-05-10T08:09:13.560052Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "T = 1000\n",
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T)\n",
    "\n",
    "n_trials = 100\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    ucb_agent = UCB1Agent(K, T)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ucb_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ucb_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('cumulative regret of UCB1')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "#plt.plot((0,T-1), (average_regret[0], average_regret[-1]), 'ro', linestyle=\"--\")\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of UCB1')"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if the rewards are not in [0,1]?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:18.537928Z",
     "start_time": "2024-05-10T08:09:18.524363Z"
    }
   },
   "source": [
    "### Exercise: repeat the previous experiments with an environment generating rewards in [0, 10], and try to make UCB1 work properly\n",
    "### Hint: Try to use a Binomial instead of a Bernoulli"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can I improve UCB1 and avoid providing T as an input?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:18.568552Z",
     "start_time": "2024-05-10T08:09:18.540332Z"
    }
   },
   "source": [
    "### Exercise: slightly modify the UCB1Agent to work without requiring T as an input, show that the empirical performances improve"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual representation of confidence intervals shrinking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:19.314071Z",
     "start_time": "2024-05-10T08:09:18.574765Z"
    }
   },
   "source": [
    "import sys \n",
    "\n",
    "p = np.array([0.25, 0.5, 0.75])\n",
    "sigma = 0.1\n",
    "T = 10000\n",
    "\n",
    "env = BernoulliEnvironment(p, T, seed)\n",
    "ucb_agent = UCB1Agent(K, T)\n",
    "n_pulls = np.zeros(K)\n",
    "\n",
    "ci_widths = np.zeros((T,K))\n",
    "\n",
    "curr_widths = np.sqrt(2*np.log(T))\n",
    "curr_average = np.zeros((T,K))\n",
    "\n",
    "for t in range(T):\n",
    "    a_t = ucb_agent.pull_arm()\n",
    "    r_t = env.round(a_t)\n",
    "    ucb_agent.update(r_t)\n",
    "\n",
    "    n_pulls[a_t] += 1\n",
    "\n",
    "    ci_widths[t,:] = np.sqrt(2*np.log(T)/n_pulls)\n",
    "\n",
    "    curr_average[t,:] = curr_average[t-1,:]\n",
    "    curr_average[t,a_t] += (r_t - curr_average[t,a_t])/n_pulls[a_t]\n",
    "\n",
    "plt.plot(curr_average+ci_widths)\n",
    "plt.title('UCBs')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylim(0.75,1)\n",
    "plt.xlim(500, 10000)\n",
    "plt.show()"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Thompson Sampling Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:19.329409Z",
     "start_time": "2024-05-10T08:09:19.318777Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "K = 3\n",
    "T = 100\n",
    "seed = 17\n",
    "\n",
    "env = BernoulliEnvironment(p, T, seed)\n",
    "env.t, env.round(0), env.t"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB1 is a deterministic algorithm, i.e., given the same sequence of rewards the chosen actions will always be the same. It has been observed that often, in practice, randomized algorithm can perform better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We thus introduce Thompson Sampling, an algorithm having similar theoretical guarantees of UCB1, but taking randomized decisions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:19.376263Z",
     "start_time": "2024-05-10T08:09:19.337462Z"
    }
   },
   "source": [
    "class TSAgent(Agent):\n",
    "    def __init__(self, K):\n",
    "        self.K = K\n",
    "        self.a_t = None\n",
    "        self.alpha, self.beta = np.ones(K), np.ones(K)\n",
    "        self.N_pulls = np.zeros(K)\n",
    "\n",
    "    def pull_arm(self):\n",
    "        theta = np.random.beta(self.alpha, self.beta)\n",
    "        self.a_t = np.argmax(theta)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, r_t):\n",
    "        self.alpha[self.a_t] += r_t\n",
    "        self.beta[self.a_t] += 1-r_t\n",
    "        self.N_pulls[self.a_t] += 1"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We estimate the pseudo-regret:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:27.919436Z",
     "start_time": "2024-05-10T08:09:19.378587Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "K = len(p)\n",
    "T = 1000\n",
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T)\n",
    "\n",
    "n_trials = 100\n",
    "\n",
    "regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    ts_agent = TSAgent(K)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ts_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ts_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "regret_per_trial = np.array(regret_per_trial)\n",
    "\n",
    "average_regret = regret_per_trial.mean(axis=0)\n",
    "regret_sd = regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), average_regret, label='Average Regret')\n",
    "plt.title('cumulative regret of TS')\n",
    "plt.fill_between(np.arange(T),\n",
    "                average_regret-regret_sd/np.sqrt(n_trials),\n",
    "                average_regret+regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3,\n",
    "                label='Uncertainty')\n",
    "#plt.plot((0,T-1), (average_regret[0], average_regret[-1]), 'ro', linestyle=\"--\")\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ts_agent.N_pulls)\n",
    "plt.title('number of pulls per arm of TS')\n",
    "plt.show()"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:27.934931Z",
     "start_time": "2024-05-10T08:09:27.924151Z"
    }
   },
   "source": [
    "ts_agent.N_pulls"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB1 vs Thompson Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare UCB1 and TS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:40.079487Z",
     "start_time": "2024-05-10T08:09:27.939256Z"
    }
   },
   "source": [
    "p = np.array([0.25, 0.5, 0.75])\n",
    "K = len(p)\n",
    "T = 1000\n",
    "expected_clairvoyant_rewards = np.repeat(p[best_action], T)\n",
    "\n",
    "n_trials = 100\n",
    "\n",
    "ucb_regret_per_trial = []\n",
    "ts_regret_per_trial = []\n",
    "\n",
    "for seed in range(n_trials):\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    ucb_agent = UCB1Agent(K, T, range=max(p)-min(p))\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ucb_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ucb_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    ucb_regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    env = BernoulliEnvironment(p, T, seed)\n",
    "    ts_agent = TSAgent(K)\n",
    "\n",
    "    agent_rewards = np.array([])\n",
    "\n",
    "    for t in range(T):\n",
    "        a_t = ts_agent.pull_arm()\n",
    "        r_t = env.round(a_t)\n",
    "        ts_agent.update(r_t)\n",
    "\n",
    "        agent_rewards = np.append(agent_rewards, r_t)\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_clairvoyant_rewards-agent_rewards)\n",
    "    ts_regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "ucb_regret_per_trial = np.array(ucb_regret_per_trial)\n",
    "ts_regret_per_trial = np.array(ts_regret_per_trial)\n",
    "\n",
    "ucb_average_regret = ucb_regret_per_trial.mean(axis=0)\n",
    "ucb_regret_sd = ucb_regret_per_trial.std(axis=0)\n",
    "\n",
    "ts_average_regret = ts_regret_per_trial.mean(axis=0)\n",
    "ts_regret_sd = ts_regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), ucb_average_regret, label='UCB1')\n",
    "plt.plot(np.arange(T), ts_average_regret, label='TS')\n",
    "plt.title('Cumulative Regrets of UCB and TS')\n",
    "plt.fill_between(np.arange(T),\n",
    "                ucb_average_regret-ucb_regret_sd/np.sqrt(n_trials),\n",
    "                ucb_average_regret+ucb_regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3)\n",
    "plt.fill_between(np.arange(T),\n",
    "                ts_average_regret-ts_regret_sd/np.sqrt(n_trials),\n",
    "                ts_average_regret+ts_regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3)\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=ucb_agent.N_pulls-ts_agent.N_pulls)\n",
    "plt.title('Difference in number of pulls per arm (UCB - TS)')\n",
    "plt.ylabel('$a$')\n",
    "plt.xlabel('$N_T(a)$')\n",
    "plt.show()\n"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: theoretical guarantees of UCB1 and TS are nearly the same. Moreover, TS theoretical guarantees can be proven by showing that this strategy implicitly does optimism, thus the underlying principles of the two algorithms are the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: TS can be used in any stochastic environment, however to move from Bernoulli distributed rewards to more general ones the algorithm needs some non-trivial adjustments. A good reference on coding TS for Gaussian rewards can be found here: https://gertjanvandenburg.com/blog/thompson_sampling/."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T08:09:40.094207Z",
     "start_time": "2024-05-10T08:09:40.082479Z"
    }
   },
   "source": [
    "### Exercise: try to compare UCB1 and TS in a GaussianEnvironment"
   ],
   "execution_count": 28,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
