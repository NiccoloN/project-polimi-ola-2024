{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Adversarial Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Adversarial Settings we generalize the process generating rewards. Instead of being sampled from a probability distribution, rewards are chosen by an adversary that can observe the bandit algorithm is facing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Adversarial Expert Setting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Expert settings, at each round, an agent chooses an arm and incurs its loss, but can observe the losses of all the other arms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, there is no exploration-exploitation trade-off dilemma, since exploration is pointless when all information is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "class AdversarialExpertEnvironment:\n",
    "    def __init__(self, loss_sequence):\n",
    "        self.loss_sequence = loss_sequence\n",
    "        self.t = 0\n",
    "\n",
    "    def round(self): # we do not need to receive a specific arm\n",
    "        l_t = self.loss_sequence[self.t, :] ## we return the whole loss vector\n",
    "        self.t+=1 \n",
    "        return l_t"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# I see the algorithm, then decide an arbitrary sequence of rewards:\n",
    "loss_seq = np.array([[0,1,0],[1,0,1],[1,1,1],[1,0,1],[0,1,0],[0,1,1],[0,0,0],[0,1,0],[1,1,0],[0,0,1]])\n",
    "loss_seq.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "env = AdversarialExpertEnvironment(loss_seq)\n",
    "env.t, env.round(), env.t"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of course, the Adversarial setting generalizes the Stochastic setting, since the adversary is free to choose to sample the rewards from a probability distribution, thus any Stochastic MAB is also a special instance of Adversarial MAB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The notion of regret change its meaning, the clairvoyant is not the agent always choosing the best action at any moment, but the agent always pulling the arm having the best cumulative reward at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "print(f'Best achievable cumulative loss: {loss_seq.min(axis=1).sum()}')\n",
    "print(f'Best achievable cumulative loss when always pulling the same arm: {loss_seq.sum(axis=0).min()}')"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will compare to the second."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to define a new clairvoyant: _Best arm in hindsight_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "best_arm = np.argmin(loss_seq.sum(axis=0))\n",
    "print(f'The best arm in hindsight is {best_arm}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "clairvoyant_losses = loss_seq[:, best_arm]\n",
    "clairvoyant_losses"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Hedge algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "class HedgeAgent:\n",
    "    def __init__(self, K, learning_rate):\n",
    "        self.K = K\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.ones(K)\n",
    "        self.x_t = np.ones(K)/K\n",
    "        self.a_t = None\n",
    "        self.t = 0\n",
    "\n",
    "    def pull_arm(self):\n",
    "        self.x_t = self.weights/sum(self.weights)\n",
    "        self.a_t = np.random.choice(np.arange(self.K), p=self.x_t)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, l_t):\n",
    "        self.weights *= np.exp(-self.learning_rate*l_t)\n",
    "        self.t += 1"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to practical reasons, we generate the losses using a probability distribution. However, in principle, we would be free to choose any sequence we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "K = 3\n",
    "loss_seq = np.zeros((T, K))\n",
    "np.random.seed(17)\n",
    "loss_seq[:,0] = np.random.binomial(n=1, p=0.7, size=T)\n",
    "loss_seq[:,1] = np.random.binomial(n=1, p=0.5, size=T)\n",
    "loss_seq[:,2] = np.random.binomial(n=1, p=0.25, size=T)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We distinguish two ways to computed regret: actual regret and regret in expectation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important Remark**: in adversarial settings, the expectation on regret is only taken w.r.t. to the randomness of the algorithm, since by definition the sequence of losses is not stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "learning_rate = np.sqrt(np.log(K)/T) #Â we set the learning rate as prescribed by the theory\n",
    "\n",
    "agent = HedgeAgent(K, learning_rate)\n",
    "env = AdversarialExpertEnvironment(loss_seq)\n",
    "\n",
    "best_arm = np.argmin(loss_seq.sum(axis=0))\n",
    "clairvoyant_losses = loss_seq[:, best_arm]\n",
    "\n",
    "# we log to different cumulative losses for the agent\n",
    "agent_losses = np.array([])\n",
    "expected_agent_losses = np.array([])\n",
    "\n",
    "for t in range(T):\n",
    "    a_t = agent.pull_arm()\n",
    "    l_t = env.round()\n",
    "    agent.update(l_t)\n",
    "    # logging\n",
    "    agent_losses = np.append(agent_losses, l_t[a_t])\n",
    "    expected_agent_losses = np.append(expected_agent_losses, np.dot(l_t, agent.x_t))\n",
    "\n",
    "plt.plot(np.cumsum(agent_losses-clairvoyant_losses), label='Actual Loss')\n",
    "plt.plot(np.cumsum(expected_agent_losses-clairvoyant_losses), label='Expected Loss')\n",
    "plt.title('Cumulative Regret of Hedge in Adversarial Expert Setting')\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "print(f'Best arm in hindsight: {best_arm}')\n",
    "print(f'Final allocation :{agent.x_t}') # the best arm is the one having more weight"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "print(f'Theoretical bound: {2*np.sqrt(T*np.log(K))}')\n",
    "print(f'Actual Total Regret {sum(agent_losses)-sum(clairvoyant_losses)}') \n",
    "print(f'Expected Total Regret {sum(expected_agent_losses)-sum(clairvoyant_losses)}') \n"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Adversarial Bandit Setting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Bandit Settings, the agent chooses one arm and can only observe the loss associated to that single arm. Thus, the feedback received from the environment is limited. This problem is intrinsically harder than the expert setting, due to the more limited information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "class AdversarialBanditEnvironment:\n",
    "    def __init__(self, loss_sequence):\n",
    "        self.loss_sequence = loss_sequence\n",
    "        self.t = 0\n",
    "\n",
    "    def round(self, a_t): # we need to receive a specific arm\n",
    "        l_t = self.loss_sequence[self.t, a_t] ## we return only the loss corresponding to the chosen arm\n",
    "        self.t+=1 \n",
    "        return l_t"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As before, the sequence of losses is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# I see the algorithm, then decide the sequence of rewards:\n",
    "loss_seq = np.array([[0,1,0],[1,0,1],[1,1,1],[1,0,1],[0,1,0],[0,1,1],[0,0,0],[0,1,0],[1,1,0],[0,0,1]])\n",
    "env = AdversarialBanditEnvironment(loss_seq)\n",
    "env.t, env.round(0), env.t"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As before, the regret is computed w.r.t. the best arm in hindsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "print(f'Best achievable cumulative loss: {loss_seq.min(axis=1).sum()}')\n",
    "print(f'Best achievable cumulative loss when always pulling the same arm: {loss_seq.sum(axis=0).min()}')"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will compare to the second."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, the problem is now harder, since we can only observe the feedback associated to the arm we choose. Thus, we need to deal with the exploration-exploitation trade-off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extending Hedge to a Bandit Setting: the EXP3 Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "class EXP3Agent:\n",
    "    def __init__(self, K, learning_rate):\n",
    "        self.K = K\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.ones(K)\n",
    "        self.a_t = None\n",
    "        self.x_t = np.ones(K)/K\n",
    "        self.N_pulls = np.zeros(K)\n",
    "        self.t = 0\n",
    "\n",
    "    def pull_arm(self):\n",
    "        self.x_t = self.weights/sum(self.weights)\n",
    "        self.a_t = np.random.choice(np.arange(self.K), p=self.x_t)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, l_t):\n",
    "        l_t_tilde = l_t/self.x_t[self.a_t]\n",
    "        self.weights[self.a_t] *= np.exp(-self.learning_rate*l_t_tilde)\n",
    "        self.N_pulls[self.a_t] += 1\n",
    "        self.t += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "T = 10000\n",
    "K = 3\n",
    "loss_seq = np.zeros((T, K))\n",
    "np.random.seed(17)\n",
    "loss_seq[:,0] = np.random.binomial(n=1, p=0.7, size=T)\n",
    "loss_seq[:,1] = np.random.binomial(n=1, p=0.5, size=T)\n",
    "loss_seq[:,2] = np.random.binomial(n=1, p=0.25, size=T)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, we can compute the received losses in two ways: actual and expected. Note that in bandit settings, differently from experts, the expected loss cannot be computed in practice, since it would require to know all the losses associated to every arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "learning_rate = np.sqrt(np.log(K)/(K*T)) #Â we set the learning rate as prescribed by the theory (different from expert setting)\n",
    "\n",
    "agent = EXP3Agent(K, learning_rate)\n",
    "env = AdversarialBanditEnvironment(loss_seq)\n",
    "\n",
    "best_arm = np.argmin(loss_seq.sum(axis=0))\n",
    "clairvoyant_losses = loss_seq[:, best_arm]\n",
    "\n",
    "agent_losses = np.array([])\n",
    "expected_agent_losses = np.array([])\n",
    "for t in range(T):\n",
    "    a_t = agent.pull_arm()\n",
    "    l_t = env.round(a_t)\n",
    "    agent.update(l_t)\n",
    "    # logging\n",
    "    agent_losses = np.append(agent_losses, l_t)\n",
    "    expected_agent_losses = np.append(expected_agent_losses,\n",
    "                                    np.dot(agent.x_t,\n",
    "                                           env.loss_sequence[t-1,:]))\n",
    "\n",
    "plt.plot(np.cumsum(agent_losses-clairvoyant_losses), label='Actual Loss')\n",
    "plt.plot(np.cumsum(expected_agent_losses-clairvoyant_losses), label='Expected Loss')\n",
    "plt.title('Cumulative Regret of Hedge in Bandit Setting')\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: the agent only observes the loss associated to the chosen arm, expected loss cannot be computed without knowing the losses from all arms (like in the expert setting). Thus, the algorithm only reacts based on the actual loss incurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "print(f'Best arm in hindsight: {best_arm}')\n",
    "print(f'Final allocation :{agent.x_t}') # the best arm is the one having more weight"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "print(f'Theoretical bound: {2*np.sqrt(T*K)}') # The theoretical bound has worsen dependence on K w.r.t. expert setting (from log(K) to sqrt(K))\n",
    "print(f'Actual Total Regret {sum(agent_losses)-sum(clairvoyant_losses)}') "
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can quantify the uncertainty on expected regret, where the only uncertainty source is algorithm's randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "learning_rate = np.sqrt(np.log(K)/(K*T)) #Â we set the learning rate as prescribed by the theory (different from expert setting)\n",
    "\n",
    "best_arm = np.argmin(loss_seq.sum(axis=0))\n",
    "clairvoyant_losses = loss_seq[:, best_arm]\n",
    "\n",
    "n_trials = 20\n",
    "\n",
    "exp3_regret_per_trial = []\n",
    "# we keep the loss sequence fixed, we will only observe uncertainty due to algorithm's randomizations\n",
    "for trial in range(n_trials):\n",
    "    agent = EXP3Agent(K, learning_rate)\n",
    "    env = AdversarialBanditEnvironment(loss_seq)\n",
    "    \n",
    "    expected_agent_losses = np.array([])\n",
    "    for t in range(T):\n",
    "        a_t = agent.pull_arm()\n",
    "        l_t = env.round(a_t)\n",
    "        agent.update(l_t)\n",
    "        # logging\n",
    "        expected_agent_losses = np.append(expected_agent_losses,\n",
    "                                        np.dot(agent.x_t,\n",
    "                                        env.loss_sequence[t-1,:]))\n",
    "\n",
    "    cumulative_regret = np.cumsum(expected_agent_losses-clairvoyant_losses)\n",
    "    exp3_regret_per_trial.append(cumulative_regret)\n",
    "\n",
    "exp3_regret_per_trial = np.array(exp3_regret_per_trial)\n",
    "\n",
    "exp3_average_regret = exp3_regret_per_trial.mean(axis=0)\n",
    "exp3_regret_sd = exp3_regret_per_trial.std(axis=0)\n",
    "\n",
    "plt.plot(np.arange(T), exp3_average_regret, label='EXP3')\n",
    "plt.title('Cumulative Regret of EXP3')\n",
    "plt.fill_between(np.arange(T),\n",
    "                exp3_average_regret-exp3_regret_sd/np.sqrt(n_trials),\n",
    "                exp3_average_regret+exp3_regret_sd/np.sqrt(n_trials),\n",
    "                alpha=0.3)\n",
    "plt.xlabel('$t$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.barh(y=['0','1','2'], width=agent.N_pulls)\n",
    "plt.title('Number of pulls per arm of EXP3')\n",
    "plt.ylabel('$a$')\n",
    "plt.xlabel('$N_T(a)$')\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
